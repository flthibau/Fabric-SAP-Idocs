# Gold Layer - Notebooks Implementation Guide

**Created:** 2025-10-27  
**Purpose:** Guide d'impl√©mentation des notebooks Gold Layer dans Fabric  
**Architecture:** Silver (shortcuts OneLake) ‚Üí Spark Notebooks ‚Üí Gold (Delta tables natives)

---

## üìÅ Structure des Fichiers

```
fabric/data-engineering/
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ gold_layer_orders_summary.py          ‚úÖ CR√â√â
‚îÇ   ‚îú‚îÄ‚îÄ gold_layer_sla_performance.py          ‚úÖ CR√â√â
‚îÇ   ‚îú‚îÄ‚îÄ gold_layer_shipments_in_transit.py     ‚úÖ CR√â√â
‚îÇ   ‚îú‚îÄ‚îÄ gold_layer_warehouse_productivity.py   ‚úÖ CR√â√â
‚îÇ   ‚îî‚îÄ‚îÄ gold_layer_revenue_recognition.py      ‚úÖ CR√â√â
‚îî‚îÄ‚îÄ pipelines/
    ‚îî‚îÄ‚îÄ pipeline_gold_layer_refresh.json       ‚úÖ CR√â√â
```

---

## üöÄ D√©ploiement dans Fabric

### √âtape 1: Uploader les Notebooks

#### Via Fabric Portal (UI)

1. **Ouvrir Workspace JAc**
   ```
   https://app.fabric.microsoft.com/groups/ad53e547-23dc-46b0-ab5f-2acbaf0eec64
   ```

2. **Pour chaque notebook:**
   - Cliquer sur `+ New` ‚Üí `Notebook`
   - Dans le notebook vide, cliquer sur le menu `...` ‚Üí `Import`
   - S√©lectionner le fichier `.py` correspondant
   - OU copier/coller directement le contenu du fichier
   - Renommer le notebook:
     * `Gold Layer - Orders Summary`
     * `Gold Layer - SLA Performance`
     * `Gold Layer - Shipments In Transit`
     * `Gold Layer - Warehouse Productivity`
     * `Gold Layer - Revenue Recognition`
   - Sauvegarder (Ctrl+S)

3. **Configurer Lakehouse par d√©faut:**
   - Dans chaque notebook, cliquer sur `Add` (en haut √† gauche)
   - S√©lectionner `Lakehouse`
   - Choisir `Lakehouse3PLAnalytics`
   - Cliquer `Add`
   - Le Lakehouse appara√Æt maintenant dans la sidebar gauche

#### Via Git (Recommand√© pour production)

```bash
# Cloner le repo dans Fabric Workspace
1. Dans Fabric Portal ‚Üí Workspace Settings ‚Üí Git integration
2. Connect to Git
3. Repository URL: https://github.com/flthibau/Fabric-SAP-Idocs
4. Branch: main
5. Path: fabric/data-engineering/notebooks
6. Sync

# Les notebooks apparaissent automatiquement dans le workspace
```

---

### √âtape 2: Tester les Notebooks Individuellement

#### Test 1: Orders Summary

```python
# Dans Fabric Notebook: Gold Layer - Orders Summary
# Cliquer sur "Run All" ou Ctrl+Shift+Enter

# V√©rifier r√©sultats attendus:
# 1. Source rows: [nombre de lignes dans idoc_orders_silver]
# 2. Gold rows computed: [nombre de jours √ó syst√®mes √ó SLA status]
# 3. Table orders_daily_summary cr√©√©e dans Tables/
# 4. MERGE completed successfully
# 5. Summary affich√©e avec total_orders, avg_sla_compliance_pct
```

**Validation:**
- Aller dans Lakehouse ‚Üí Tables ‚Üí V√©rifier `orders_daily_summary` existe
- Cliquer sur la table ‚Üí Preview data
- V√©rifier colonnes: order_day, sap_system, sla_status, total_orders, total_revenue, sla_compliance_pct

**Dur√©e attendue:** 2-5 minutes

---

#### Test 2: SLA Performance

```python
# Notebook: Gold Layer - SLA Performance
# Run All

# V√©rifier:
# 1. Orders + Shipments jointure r√©ussie
# 2. SLA metrics calcul√©s (processing_days, sla_compliance, on_time_delivery)
# 3. Table sla_performance cr√©√©e
# 4. Summary par sla_compliance affich√©
```

**Validation:**
- Table `sla_performance` existe
- Colonnes: order_number, processing_days, sla_compliance, is_critical, on_time_delivery
- Data quality: Aucune ligne avec processing_days < 0

**Dur√©e attendue:** 3-7 minutes (JOIN co√ªteux)

---

#### Test 3: Shipments In Transit

```python
# Notebook: Gold Layer - Shipments In Transit
# Run All

# V√©rifier:
# 1. Filtre In Transit appliqu√© (actual_ship_date NOT NULL, actual_delivery_date NULL)
# 2. ETA et delay metrics calcul√©s
# 3. Table shipments_in_transit OVERWRITTEN (pas merge)
# 4. Top delayed shipments affich√©s
```

**Validation:**
- Table `shipments_in_transit` existe
- Snapshot temps r√©el uniquement (pas d'historique)
- Colonnes: days_in_transit, days_until_planned_delivery, delay_status, priority

**Dur√©e attendue:** 1-3 minutes

---

#### Test 4: Warehouse Productivity

```python
# Notebook: Gold Layer - Warehouse Productivity
# Run All

# V√©rifier:
# 1. Agr√©gations par jour √ó warehouse √ó movement_type
# 2. Productivit√© calcul√©e (movements_per_hour, quantity_per_hour)
# 3. Exception rate calcul√©
# 4. Table warehouse_productivity_daily cr√©√©e
```

**Validation:**
- Table `warehouse_productivity_daily` existe
- Colonnes: movement_day, warehouse_id, quantity_per_hour, exception_rate_pct, performance_status
- Productivity target = 100

**Dur√©e attendue:** 2-5 minutes

---

#### Test 5: Revenue Recognition

```python
# Notebook: Gold Layer - Revenue Recognition
# Run All

# V√©rifier:
# 1. Agr√©gations par jour √ó customer
# 2. Aging buckets calcul√©s (Current, 1-30, 31-60, 61-90, 90+)
# 3. DSO (Days Sales Outstanding) calcul√©
# 4. Collection efficiency calcul√©
# 5. Table revenue_recognition_realtime cr√©√©e
```

**Validation:**
- Table `revenue_recognition_realtime` existe
- Colonnes: invoice_day, customer_id, total_revenue, total_paid, total_due, aging buckets, collection_efficiency_pct
- Data quality: total_paid + total_due = total_revenue (¬± 1 cent)

**Dur√©e attendue:** 2-5 minutes

---

### √âtape 3: Cr√©er le Pipeline d'Orchestration

#### Via Fabric Portal

1. **Cr√©er nouveau Pipeline:**
   ```
   Workspace JAc ‚Üí + New ‚Üí Data pipeline
   Nom: "Gold Layer - Daily Refresh"
   ```

2. **Ajouter 5 activit√©s Notebook (parall√®le):**
   - Dans le canvas, glisser-d√©poser `Notebook` (5 fois)
   - Configurer chaque activit√©:
     
     **Activit√© 1:**
     - Name: `Orders Daily Summary`
     - Notebook: `Gold Layer - Orders Summary`
     - Lakehouse: `Lakehouse3PLAnalytics`
     - Timeout: 10 minutes
     - Retry: 2
     
     **Activit√© 2:**
     - Name: `SLA Performance`
     - Notebook: `Gold Layer - SLA Performance`
     - Timeout: 10 minutes
     - Retry: 2
     
     **Activit√© 3:**
     - Name: `Shipments In Transit`
     - Notebook: `Gold Layer - Shipments In Transit`
     - Timeout: 5 minutes
     - Retry: 2
     
     **Activit√© 4:**
     - Name: `Warehouse Productivity`
     - Notebook: `Gold Layer - Warehouse Productivity`
     - Timeout: 10 minutes
     - Retry: 2
     
     **Activit√© 5:**
     - Name: `Revenue Recognition`
     - Notebook: `Gold Layer - Revenue Recognition`
     - Timeout: 10 minutes
     - Retry: 2

3. **Ajouter activit√© Web (trigger Purview scan):**
   - Apr√®s les 5 notebooks (d√©pendance: "On Success")
   - Name: `Trigger Purview Scan`
   - URL: `https://stpurview.scan.purview.azure.com/datasources/Fabric-JAc/scans/Scan-DKT/run?api-version=2022-07-01-preview`
   - Method: `POST`
   - Authentication: `Managed Identity`
   - Body:
     ```json
     {
       "scanLevel": "Full"
     }
     ```

4. **Configurer Schedule Trigger:**
   - Dans Pipeline ‚Üí Settings ‚Üí Triggers
   - + New ‚Üí Schedule
   - Name: `Daily Refresh 2AM`
   - Recurrence: Daily
   - Time: 02:00 AM (UTC)
   - Start date: Aujourd'hui
   - Activer le trigger

5. **Sauvegarder et Publier:**
   - Save ‚Üí Publish

---

### √âtape 4: Ex√©cution Manuelle Initiale

1. **D√©clencher pipeline:**
   ```
   Pipeline ‚Üí Run
   ```

2. **Monitorer l'ex√©cution:**
   - Aller dans `Monitor` (sidebar gauche)
   - Pipeline runs ‚Üí Voir le run en cours
   - Cliquer sur le run pour voir d√©tails
   - V√©rifier que les 5 notebooks s'ex√©cutent en parall√®le
   - Attendre succ√®s (dur√©e estim√©e: 10-15 minutes)

3. **V√©rifier r√©sultats:**
   - Lakehouse ‚Üí Tables ‚Üí Devrait voir **10 tables total**:
     * 5 shortcuts (Bronze + Silver): idoc_raw, idoc_orders_silver, idoc_shipments_silver, idoc_warehouse_silver, idoc_invoices_silver
     * 5 tables Delta natives (Gold): orders_daily_summary, sla_performance, shipments_in_transit, warehouse_productivity_daily, revenue_recognition_realtime

4. **Tester queries SQL:**
   ```sql
   -- Dans Lakehouse SQL Endpoint
   
   -- Liste toutes les tables
   SHOW TABLES;
   
   -- Test table Gold - Orders Summary
   SELECT * FROM orders_daily_summary
   WHERE order_day >= CURRENT_DATE - INTERVAL 7 DAYS
   ORDER BY order_day DESC, total_orders DESC
   LIMIT 100;
   
   -- Test table Gold - SLA Performance
   SELECT 
       sla_compliance,
       COUNT(*) as order_count,
       ROUND(AVG(processing_days), 2) as avg_processing_days,
       COUNT(CASE WHEN is_critical THEN 1 END) as critical_orders
   FROM sla_performance
   GROUP BY sla_compliance
   ORDER BY order_count DESC;
   
   -- Test table Gold - Shipments In Transit
   SELECT * FROM shipments_in_transit
   WHERE delay_status = 'Delayed'
   ORDER BY days_delayed DESC, shipment_value DESC
   LIMIT 20;
   
   -- Test table Gold - Warehouse Productivity
   SELECT 
       warehouse_id,
       ROUND(AVG(quantity_per_hour), 2) as avg_productivity,
       ROUND(AVG(exception_rate_pct), 2) as avg_exception_rate,
       COUNT(*) as total_days
   FROM warehouse_productivity_daily
   WHERE movement_day >= CURRENT_DATE - INTERVAL 30 DAYS
   GROUP BY warehouse_id
   ORDER BY avg_productivity DESC;
   
   -- Test table Gold - Revenue Recognition
   SELECT 
       customer_id,
       SUM(total_revenue) as total_revenue,
       SUM(aging_90_plus) as at_risk_amount,
       ROUND(AVG(collection_efficiency_pct), 2) as avg_collection_efficiency
   FROM revenue_recognition_realtime
   WHERE invoice_day >= CURRENT_DATE - INTERVAL 90 DAYS
   GROUP BY customer_id
   ORDER BY total_revenue DESC
   LIMIT 10;
   ```

---

## üîç Purview Integration

### √âtape 5: D√©clencher Scan Purview

Le scan Purview est d√©clench√© automatiquement par le pipeline apr√®s cr√©ation des tables Gold.

**V√©rification manuelle si n√©cessaire:**

```bash
cd governance/purview
python purview_automation.py
```

### √âtape 6: V√©rifier Assets D√©couverts

1. **Via Portal Purview:**
   ```
   https://web.purview.azure.com/resource/stpurview
   Data Map ‚Üí Sources ‚Üí Fabric-JAc ‚Üí Browse assets
   ```

2. **Via Script Python:**
   ```bash
   cd governance/purview
   python list_discovered_assets.py
   ```

3. **R√©sultats attendus:**
   - **10 tables d√©couvertes:**
     * 1 Bronze: `idoc_raw`
     * 4 Silver: `idoc_orders_silver`, `idoc_shipments_silver`, `idoc_warehouse_silver`, `idoc_invoices_silver`
     * 5 Gold: `orders_daily_summary`, `sla_performance`, `shipments_in_transit`, `warehouse_productivity_daily`, `revenue_recognition_realtime`
   
   - **M√©tadonn√©es pour chaque table:**
     * Schema complet (colonnes, types)
     * Qualified name
     * Collection assignment
     * Lakehouse parent

---

## üìä Cr√©ation Data Product Purview

### √âtape 7: Cr√©er Business Domain

**Via Portal Purview:**

1. Data Catalog ‚Üí Domains ‚Üí + New Domain
2. Configuration:
   - Name: `3PL Real-Time Analytics`
   - Description: "Business Domain for Third-Party Logistics (3PL) real-time analytics covering order management, shipment tracking, warehouse operations, and financial processes"
   - Owner: [Votre email]
   - Experts: [√âquipe 3PL]
   - Collection: Bronze (ou root)
3. Create

### √âtape 8: Cr√©er Data Product

**Dans le Business Domain cr√©√©:**

1. + New Data Product
2. Configuration:
   - Name: `3PL Real-Time Analytics Data Product`
   - Description: "Real-time analytics platform for 3PL operations combining SAP IDoc ingestion, streaming processing, and analytical reporting"
   - Owner: [Votre email]
   
3. **Input Ports:**
   - Event Hub: `eh-idoc-flt8076/idoc-events`
   - SAP System: `S4HPRD Client 100`
   
4. **Output Ports:**
   - Lakehouse Delta Tables: `Lakehouse3PLAnalytics`
   - GraphQL API (planned)
   - Power BI (planned)
   
5. **Data Assets (ajouter les 10 tables):**
   - Bronze Layer:
     * `idoc_raw`
   - Silver Layer:
     * `idoc_orders_silver`
     * `idoc_shipments_silver`
     * `idoc_warehouse_silver`
     * `idoc_invoices_silver`
   - Gold Layer:
     * `orders_daily_summary`
     * `sla_performance`
     * `shipments_in_transit`
     * `warehouse_productivity_daily`
     * `revenue_recognition_realtime`
   
6. **Business Glossary:**
   - Lier les 6 termes existants:
     * Order
     * Shipment
     * SLA Compliance %
     * On-Time Delivery %
     * Warehouse Productivity
     * Days Sales Outstanding (DSO)
   
7. **KPIs:**
   - SLA Compliance % (target >95%)
   - On-Time Delivery % (target >98%)
   - Warehouse Productivity (target >100 pallets/hour)
   - Days Sales Outstanding (target <30 days)

8. Create

---

## ‚úÖ Validation Compl√®te

### Checklist Final

- [ ] **Notebooks cr√©√©s (5):**
  - [ ] Orders Summary
  - [ ] SLA Performance
  - [ ] Shipments In Transit
  - [ ] Warehouse Productivity
  - [ ] Revenue Recognition

- [ ] **Notebooks test√©s individuellement:**
  - [ ] Tous s'ex√©cutent sans erreur
  - [ ] Tables Delta cr√©√©es dans Lakehouse
  - [ ] Data quality checks passent

- [ ] **Pipeline cr√©√©:**
  - [ ] 5 notebooks orchestr√©s
  - [ ] Trigger Purview scan configur√©
  - [ ] Schedule trigger actif (2 AM daily)

- [ ] **Pipeline ex√©cut√© avec succ√®s:**
  - [ ] Run manuel initial r√©ussi
  - [ ] 10 tables visibles dans Lakehouse
  - [ ] Queries SQL fonctionnelles

- [ ] **Purview Scan r√©ussi:**
  - [ ] 10 tables d√©couvertes
  - [ ] M√©tadonn√©es compl√®tes (schemas)
  - [ ] Assets visibles dans Data Map

- [ ] **Data Product cr√©√©:**
  - [ ] Business Domain cr√©√©
  - [ ] Data Product configur√©
  - [ ] 10 tables associ√©es
  - [ ] Business Glossary li√©
  - [ ] KPIs document√©s

---

## üéØ R√©sultat Final

```
Purview Data Product "3PL Real-Time Analytics"
‚îÇ
‚îú‚îÄ‚îÄ Input Ports
‚îÇ   ‚îú‚îÄ‚îÄ SAP S/4HANA (S4HPRD Client 100)
‚îÇ   ‚îî‚îÄ‚îÄ Azure Event Hub (eh-idoc-flt8076/idoc-events)
‚îÇ
‚îú‚îÄ‚îÄ Data Assets (10 tables gouvern√©es)
‚îÇ   ‚îú‚îÄ‚îÄ Bronze Layer (1)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ idoc_raw
‚îÇ   ‚îú‚îÄ‚îÄ Silver Layer (4)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ idoc_orders_silver
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ idoc_shipments_silver
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ idoc_warehouse_silver
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ idoc_invoices_silver
‚îÇ   ‚îî‚îÄ‚îÄ Gold Layer (5) ‚ú® NOUVEAU
‚îÇ       ‚îú‚îÄ‚îÄ orders_daily_summary
‚îÇ       ‚îú‚îÄ‚îÄ sla_performance
‚îÇ       ‚îú‚îÄ‚îÄ shipments_in_transit
‚îÇ       ‚îú‚îÄ‚îÄ warehouse_productivity_daily
‚îÇ       ‚îî‚îÄ‚îÄ revenue_recognition_realtime
‚îÇ
‚îú‚îÄ‚îÄ Business Glossary (6 termes)
‚îÇ   ‚îú‚îÄ‚îÄ Order
‚îÇ   ‚îú‚îÄ‚îÄ Shipment
‚îÇ   ‚îú‚îÄ‚îÄ SLA Compliance %
‚îÇ   ‚îú‚îÄ‚îÄ On-Time Delivery %
‚îÇ   ‚îú‚îÄ‚îÄ Warehouse Productivity
‚îÇ   ‚îî‚îÄ‚îÄ Days Sales Outstanding (DSO)
‚îÇ
‚îú‚îÄ‚îÄ Data Quality Rules (30+)
‚îÇ   ‚îú‚îÄ‚îÄ Bronze: 5 rules
‚îÇ   ‚îú‚îÄ‚îÄ Silver: 15 rules
‚îÇ   ‚îî‚îÄ‚îÄ Gold: 10+ rules
‚îÇ
‚îú‚îÄ‚îÄ Lineage (complet)
‚îÇ   ‚îî‚îÄ‚îÄ SAP ‚Üí Event Hub ‚Üí Eventhouse ‚Üí OneLake ‚Üí Lakehouse Silver ‚Üí Spark ‚Üí Lakehouse Gold ‚úÖ
‚îÇ
‚îî‚îÄ‚îÄ Output Ports
    ‚îú‚îÄ‚îÄ Lakehouse3PLAnalytics (10 tables Delta)
    ‚îú‚îÄ‚îÄ GraphQL API (planned)
    ‚îî‚îÄ‚îÄ Power BI (planned)
```

---

## üìà Monitoring & Maintenance

### Dashboard Monitoring (Fabric)

```
Workspace ‚Üí Monitor ‚Üí Pipeline runs
- Voir historique ex√©cutions
- Dur√©e moyenne: 10-15 minutes
- Taux de succ√®s: >95%
- Alertes si √©chec
```

### Data Quality Monitoring

```sql
-- Dans Lakehouse SQL Endpoint

-- V√©rifier freshness des tables Gold
SELECT 
    'orders_daily_summary' as table_name,
    MAX(order_day) as latest_day,
    DATEDIFF(day, MAX(order_day), CURRENT_DATE) as days_old
FROM orders_daily_summary
UNION ALL
SELECT 
    'sla_performance',
    MAX(order_date),
    DATEDIFF(day, MAX(order_date), CURRENT_DATE)
FROM sla_performance
UNION ALL
SELECT 
    'warehouse_productivity_daily',
    MAX(movement_day),
    DATEDIFF(day, MAX(movement_day), CURRENT_DATE)
FROM warehouse_productivity_daily;

-- Alerte si days_old > 1 (donn√©es pas rafra√Æchies)
```

### Maintenance Mensuelle

- [ ] Review performance notebooks (dur√©e ex√©cution)
- [ ] Optimize Delta tables (OPTIMIZE + ZORDER)
- [ ] Vacuum old files (>7 jours)
- [ ] Review Data Quality failures
- [ ] Update documentation si changements

---

## üÜò Troubleshooting

### Probl√®me: Notebook √©choue avec "Table not found"

**Cause:** Shortcut Silver pas encore cr√©√© ou OneLake pas sync

**Solution:**
```bash
# V√©rifier shortcuts dans Lakehouse
Lakehouse ‚Üí Tables ‚Üí Devrait voir 5 shortcuts (icone lien)

# Si manquant, recr√©er shortcuts OneLake
# Ou attendre sync OneLake (2-5 minutes apr√®s activation)
```

### Probl√®me: MERGE √©choue avec "Schema mismatch"

**Cause:** Schema Silver a chang√©

**Solution:**
```python
# Dans notebook, ajouter option:
df_gold.write \
    .format("delta") \
    .mode("overwrite") \
    .option("overwriteSchema", "true")  # Force schema update
    .saveAsTable(TARGET_TABLE)
```

### Probl√®me: Pipeline timeout

**Cause:** Volum√©trie trop √©lev√©e

**Solution:**
```json
// Augmenter timeout dans pipeline
{
  "policy": {
    "timeout": "0.00:30:00",  // 30 minutes au lieu de 10
    "retry": 2
  }
}
```

### Probl√®me: Purview scan ne d√©couvre pas toutes les tables

**Cause:** Scan partiel ou cache

**Solution:**
```bash
# Trigger full scan manuel
cd governance/purview
python purview_automation.py --scan-level Full

# Attendre 5-10 minutes
# Re-v√©rifier assets
python list_discovered_assets.py
```

---

## üìö Ressources

- **Documentation compl√®te:** `governance/LAKEHOUSE-GOLD-LAYER-APPROACH.md`
- **Architecture analysis:** `governance/DATA-PRODUCT-ARCHITECTURE-ANALYSIS.md`
- **Data Quality rules:** `governance/DATA-QUALITY-RULES.md`
- **Lineage:** `governance/DATA-LINEAGE.md`
- **Business Glossary:** `governance/BUSINESS-GLOSSARY.md`

---

**Prochaine √©tape:** D√©ployer les notebooks dans Fabric et ex√©cuter le pipeline initial !
