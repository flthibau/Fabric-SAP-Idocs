# Microsoft Fabric Components

## Overview

# Microsoft Fabric - Ingestion et Analyse des IDocs SAP

Ce dossier contient toute la configuration et les ressources pour ingÃ©rer et analyser les messages IDoc SAP dans Microsoft Fabric Real-Time Intelligence.

## ğŸ“‹ Vue d'ensemble

Cette solution permet de :
- ğŸ“¥ **IngÃ©rer** les messages IDoc depuis Azure Event Hub vers Fabric
- ğŸ“Š **Analyser** les donnÃ©es en temps rÃ©el avec KQL Database
- ğŸ“ˆ **Visualiser** les mÃ©triques avec Power BI et dashboards Fabric
- ğŸ”” **Alerter** sur les anomalies et exceptions

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Simulateur     â”‚â”€â”€â”€â”€â”€â–¶â”‚  Azure Event Hub â”‚â”€â”€â”€â”€â”€â–¶â”‚  Fabric Eventstream     â”‚
â”‚  Python         â”‚      â”‚  idoc-events     â”‚      â”‚  evs-sap-idoc-ingest    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                               â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚                                              â”‚
                              â–¼                                              â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  KQL Database       â”‚                   â”‚   Lakehouse          â”‚
                     â”‚  kqldb-sap-idoc     â”‚                   â”‚   lh-sap-idoc        â”‚
                     â”‚  - Analyse RTI      â”‚                   â”‚   - Stockage LT      â”‚
                     â”‚  - RequÃªtes KQL     â”‚                   â”‚   - Archives         â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚                     â”‚
                     â–¼                     â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Power BI       â”‚   â”‚  Data Activator  â”‚
            â”‚  Dashboards     â”‚   â”‚  Alertes         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Structure du dossier

```
fabric/
â”œâ”€â”€ README.md                      # Ce fichier
â”œâ”€â”€ README_KQL_QUERIES.md          # Collection de requÃªtes KQL
â”œâ”€â”€ eventstream/
â”‚   â”œâ”€â”€ EVENTSTREAM_SETUP.md       # Guide configuration Eventstream
â”‚   â””â”€â”€ setup-fabric-connection.ps1 # Script de prÃ©paration
â”œâ”€â”€ data-engineering/
â”‚   â”œâ”€â”€ notebooks/                 # Notebooks Fabric (transformations)
â”‚   â””â”€â”€ pipelines/                 # Data pipelines
â””â”€â”€ warehouse/
    â””â”€â”€ schema/                    # SchÃ©mas de tables
```

## ğŸš€ DÃ©marrage rapide

### PrÃ©requis

âœ… Azure Event Hub dÃ©ployÃ© et opÃ©rationnel (`eh-idoc-flt8076/idoc-events`)  
âœ… Messages IDoc envoyÃ©s depuis le simulateur  
âœ… Workspace Microsoft Fabric avec capacitÃ© F64 (ou supÃ©rieure)  
âœ… Permissions : Contributor sur workspace + Event Hubs Data Receiver  

### Ã‰tape 1 : PrÃ©parer la connexion

ExÃ©cutez le script de configuration pour crÃ©er le consumer group et vÃ©rifier les permissions :

```powershell
cd fabric\eventstream
.\setup-fabric-connection.ps1
```

Ce script :
- CrÃ©e le consumer group `fabric-consumer`
- VÃ©rifie/assigne les permissions RBAC
- Affiche les informations de connexion

### Ã‰tape 2 : CrÃ©er l'Eventstream

1. Ouvrez votre workspace Fabric
2. CrÃ©ez un **Eventstream** : `evs-sap-idoc-ingest`
3. Suivez le guide dÃ©taillÃ© : [`eventstream/EVENTSTREAM_SETUP.md`](./eventstream/EVENTSTREAM_SETUP.md)

### Ã‰tape 3 : CrÃ©er la KQL Database

1. Dans Fabric, crÃ©ez une **KQL Database** : `kqldb-sap-idoc`
2. Ajoutez une destination depuis l'Eventstream vers cette database
3. CrÃ©ez la table `idoc_raw` avec le schema dÃ©tectÃ©

### Ã‰tape 4 : Analyser les donnÃ©es

Utilisez les requÃªtes KQL du fichier [`README_KQL_QUERIES.md`](./README_KQL_QUERIES.md) :

```kql
// AperÃ§u des derniers messages
idoc_raw
| take 10
| order by timestamp desc
| project timestamp, idoc_type, message_type, sap_system
```

```kql
// Distribution des types d'IDoc
idoc_raw
| summarize count() by message_type
| render piechart
```

## ğŸ“Š Cas d'usage

### 1. Monitoring en temps rÃ©el

**Objectif** : Surveiller le flux de messages SAP en temps rÃ©el

**RequÃªte KQL** :
```kql
idoc_raw
| where timestamp > ago(5m)
| summarize Messages = count() by bin(timestamp, 30s), message_type
| render timechart
```

**Dashboard** : CrÃ©ez un tile Fabric rafraÃ®chi toutes les 30 secondes

---

### 2. Analyse des performances

**Objectif** : Identifier les goulots d'Ã©tranglement

**RequÃªte KQL** :
```kql
idoc_raw
| extend ingestion_time = ingestion_time()
| extend latency_seconds = datetime_diff('second', ingestion_time, todatetime(timestamp))
| summarize 
    Latence_P50 = percentile(latency_seconds, 50),
    Latence_P95 = percentile(latency_seconds, 95)
    by bin(timestamp, 5m)
| render timechart
```

---

### 3. DÃ©tection d'anomalies

**Objectif** : Alerter sur les messages en erreur

**RequÃªte KQL** :
```kql
idoc_raw
| where tostring(control.status) != "03"
| project timestamp, message_type, docnum=control.docnum, status=control.status
| order by timestamp desc
```

**Alerte** : Configurez Data Activator / Reflex pour dÃ©clencher une alerte Teams/Email

---

### 4. Analyse mÃ©tier

**Objectif** : Analyser les commandes par client

**RequÃªte KQL** :
```kql
idoc_raw
| where message_type == "ORDERS05"
| extend NumCommande = control.docnum
| extend Client = tostring(data.E1EDK01[0].BELNR)
| summarize Total_Commandes = count() by Client
| top 10 by Total_Commandes desc
| render columnchart
```

---

## ğŸ”§ Configuration avancÃ©e

### Lakehouse pour archivage

Pour un stockage long terme, ajoutez une destination Lakehouse :

1. CrÃ©ez un **Lakehouse** : `lh-sap-idoc`
2. Depuis l'Eventstream, ajoutez destination â†’ Lakehouse
3. Table : `idoc_events` (mode Append)
4. Partitioning : Par date (annÃ©e/mois/jour)

### Data pipeline pour transformations

Pour transformer les donnÃ©es avant stockage :

1. CrÃ©ez un **Data Pipeline**
2. Source : KQL Database `idoc_raw`
3. Transformations :
   - Extraction des champs mÃ©tier (numÃ©ros de commande, montants)
   - Enrichissement (lookup tables, conversions)
   - AgrÃ©gations
4. Destination : Warehouse ou Lakehouse

### Power BI Real-time

CrÃ©ez un rapport Power BI connectÃ© au KQL Database :

1. Power BI Desktop â†’ Get Data â†’ **KQL Database**
2. Connexion : `kqldb-sap-idoc`
3. Mode : **DirectQuery** (pour temps rÃ©el)
4. CrÃ©ez des visuels (cartes, graphiques, tableaux)
5. Publiez sur Fabric avec refresh automatique

---

## ğŸ“– Documentation

| Fichier | Description |
|---------|-------------|
| [EVENTSTREAM_SETUP.md](./eventstream/EVENTSTREAM_SETUP.md) | Guide complet configuration Eventstream |
| [README_KQL_QUERIES.md](./README_KQL_QUERIES.md) | Collection de 50+ requÃªtes KQL |
| [setup-fabric-connection.ps1](./eventstream/setup-fabric-connection.ps1) | Script PowerShell de prÃ©paration |

## ğŸ” Troubleshooting

### âŒ Eventstream ne reÃ§oit pas de donnÃ©es

**Causes possibles** :
- Consumer group partagÃ© avec le CLI reader
- Permissions RBAC manquantes
- Event Hub vide

**Solution** :
```powershell
# CrÃ©er un consumer group dÃ©diÃ©
.\eventstream\setup-fabric-connection.ps1

# VÃ©rifier qu'il y a des messages
cd ..\simulator
python read_eventhub.py --max 5
```

---

### âŒ Erreur de parsing JSON

**Cause** : Format de donnÃ©es incorrect dans la source

**Solution** :
1. Dans Eventstream, Ã©ditez la source
2. Data format : **JSON**
3. Testez avec un message : `python read_eventhub.py --max 1 --details`

---

### âŒ Latence Ã©levÃ©e

**Cause** : Consumer lag, partition skew

**Solution** :
```kql
// VÃ©rifier la latence
idoc_raw
| extend ingestion_time = ingestion_time()
| extend latency = datetime_diff('second', ingestion_time, todatetime(timestamp))
| summarize avg(latency), percentile(latency, 95)
```

Augmentez le nombre de partitions si nÃ©cessaire.

---

## ğŸ¯ Prochaines Ã©tapes

1. âœ… **Configurer l'Eventstream** â†’ [`EVENTSTREAM_SETUP.md`](./eventstream/EVENTSTREAM_SETUP.md)
2. ğŸ“Š **Analyser avec KQL** â†’ [`README_KQL_QUERIES.md`](./README_KQL_QUERIES.md)
3. ğŸ“ˆ **CrÃ©er un dashboard Power BI** temps rÃ©el
4. ğŸ”” **Configurer des alertes** avec Data Activator
5. ğŸ—„ï¸ **Archiver dans Lakehouse** pour le long terme
6. ğŸ”„ **Transformer avec pipelines** pour enrichissement

---

## ğŸ“š Ressources

- [Microsoft Fabric - Documentation officielle](https://learn.microsoft.com/fabric/)
- [Eventstream - Guide](https://learn.microsoft.com/fabric/real-time-intelligence/event-streams/overview)
- [KQL Database - Tutorial](https://learn.microsoft.com/fabric/real-time-intelligence/create-database)
- [KQL Query Language](https://learn.microsoft.com/azure/data-explorer/kusto/query/)
- [Power BI Real-time](https://learn.microsoft.com/fabric/real-time-intelligence/power-bi-data-connector)

---

## ğŸ’¡ Support

Pour toute question ou problÃ¨me :
1. Consultez la section Troubleshooting ci-dessus
2. VÃ©rifiez les logs dans Eventstream (onglet Metrics)
3. Testez la connexion avec le CLI : `python ..\simulator\read_eventhub.py`
- Eventstream configuration
- Data Engineering pipelines (Spark)
- SQL Warehouse schema

## Structure

```
fabric/
â”œâ”€â”€ eventstream/
â”‚   â””â”€â”€ eventstream-config.json       # Eventstream configuration
â”œâ”€â”€ data-engineering/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â”œâ”€â”€ bronze_to_silver.ipynb    # Bronze â†’ Silver transformation
â”‚   â”‚   â”œâ”€â”€ silver_to_gold.ipynb      # Silver â†’ Gold transformation
â”‚   â”‚   â””â”€â”€ data_quality_checks.ipynb # Quality validation
â”‚   â””â”€â”€ pipelines/
â”‚       â”œâ”€â”€ ingestion_pipeline.json   # Main ingestion pipeline
â”‚       â””â”€â”€ transformation_pipeline.json
â””â”€â”€ warehouse/
    â””â”€â”€ schema/
        â”œâ”€â”€ bronze_tables.sql         # Bronze layer DDL
        â”œâ”€â”€ silver_tables.sql         # Silver layer DDL
        â”œâ”€â”€ gold_dimensions.sql       # Gold dimension tables
        â””â”€â”€ gold_facts.sql            # Gold fact tables
```

## Prerequisites

- Microsoft Fabric capacity (F64 or higher recommended)
- Fabric workspace created
- Lakehouse created: `sap-idoc-lakehouse`
- SQL Warehouse created: `sap-3pl-warehouse`
- Service Principal or User account with permissions

## Setup Instructions

### 1. Eventstream Setup

1. Navigate to your Fabric workspace
2. Create a new Eventstream: `sap-idoc-ingest`
3. Configure source (Event Hub)
4. Apply transformation from `eventstream/eventstream-config.json`
5. Set destination to Lakehouse

### 2. Lakehouse Configuration

```bash
# Create Lakehouse using Fabric API or UI
# Name: sap-idoc-lakehouse
```

Run DDL scripts in order:
1. `warehouse/schema/bronze_tables.sql`
2. `warehouse/schema/silver_tables.sql`
3. `warehouse/schema/gold_dimensions.sql`
4. `warehouse/schema/gold_facts.sql`

### 3. Data Engineering Pipelines

Import notebooks:
1. Upload notebooks to Fabric workspace
2. Attach to Lakehouse
3. Configure Spark settings

Schedule pipelines:
- Bronze to Silver: Every 5 minutes
- Silver to Gold: Every 15 minutes
- Data Quality: Hourly

## Medallion Architecture

### Bronze Layer (Raw)
- Purpose: Store raw IDoc messages
- Format: Delta Lake
- Retention: 90 days
- Partitioning: By date and IDoc type

### Silver Layer (Cleansed)
- Purpose: Cleaned and normalized data
- Format: Delta Lake
- Retention: 2 years
- Features: Deduplication, validation, standardization

### Gold Layer (Analytics)
- Purpose: Business-ready dimensional model
- Format: SQL Warehouse tables
- Retention: 7 years
- Design: Star schema with dimensions and facts

## Key Tables

| Table | Layer | Description |
|-------|-------|-------------|
| `bronze_idocs` | Bronze | Raw IDoc messages |
| `silver_shipments` | Silver | Cleansed shipment data |
| `silver_deliveries` | Silver | Cleansed delivery data |
| `dim_customer` | Gold | Customer dimension |
| `dim_location` | Gold | Location dimension |
| `fact_shipment` | Gold | Shipment fact table |

## Data Transformation Flow

```
Event Hub
    â†“
Eventstream (validation, enrichment)
    â†“
bronze_idocs (raw storage)
    â†“
Spark Notebook: bronze_to_silver
    â†“
silver_* tables (cleansed data)
    â†“
Spark Notebook: silver_to_gold
    â†“
dim_* & fact_* tables (analytics)
```

## Monitoring

- Use Fabric Monitoring for Eventstream metrics
- Check pipeline execution history
- Monitor Delta Lake table metrics
- Set up alerts for pipeline failures

## Performance Optimization

### Delta Lake Optimization

```sql
-- Optimize tables regularly
OPTIMIZE bronze_idocs ZORDER BY (idoc_type, processing_date);
OPTIMIZE silver_shipments ZORDER BY (customer_id, ship_date);

-- Vacuum old versions
VACUUM bronze_idocs RETAIN 168 HOURS;
VACUUM silver_shipments RETAIN 168 HOURS;
```

### Spark Configuration

```python
# Recommended Spark settings for notebooks
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")
spark.conf.set("spark.databricks.delta.autoCompact.enabled", "true")
```

## Troubleshooting

### Eventstream Issues
- Check Event Hub connectivity
- Verify schema validation rules
- Review error logs in monitoring

### Pipeline Failures
- Check Spark logs
- Verify table permissions
- Ensure sufficient capacity

### Performance Issues
- Review partition strategy
- Optimize Delta tables
- Increase Spark compute resources

## Development Workflow

1. Develop transformations in notebooks
2. Test with sample data
3. Validate data quality
4. Deploy to production workspace
5. Schedule pipelines

## Contributing

When modifying Fabric components:
1. Test in development workspace
2. Document changes in notebook markdown
3. Update this README
4. Create backup of current configuration
5. Deploy changes incrementally

## Support

- Fabric Documentation: https://learn.microsoft.com/fabric/
- Internal Support: data-engineering@company.com
