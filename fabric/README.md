# Microsoft Fabric Components# Microsoft Fabric Components



## Overview## Overview



This folder contains all configuration and resources for ingesting and analyzing SAP IDoc messages in Microsoft Fabric Real-Time Intelligence.# Microsoft Fabric - Ingestion et Analyse des IDocs SAP



## ðŸ“‹ SummaryCe dossier contient toute la configuration et les ressources pour ingÃ©rer et analyser les messages IDoc SAP dans Microsoft Fabric Real-Time Intelligence.



This solution enables:## ðŸ“‹ Vue d'ensemble

- ðŸ“¥ **Ingest** IDoc messages from Azure Event Hub to Fabric

- ðŸ“Š **Analyze** real-time data with KQL DatabaseCette solution permet de :

- ðŸ“ˆ **Visualize** metrics with Power BI and Fabric dashboards- ðŸ“¥ **IngÃ©rer** les messages IDoc depuis Azure Event Hub vers Fabric

- ðŸ”” **Alert** on anomalies and exceptions- ðŸ“Š **Analyser** les donnÃ©es en temps rÃ©el avec KQL Database

- ðŸ“ˆ **Visualiser** les mÃ©triques avec Power BI et dashboards Fabric

## ðŸ—ï¸ Architecture- ðŸ”” **Alerter** sur les anomalies et exceptions



```## ðŸ—ï¸ Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚  Python         â”‚â”€â”€â”€â”€â”€â–¶â”‚  Azure Event Hub â”‚â”€â”€â”€â”€â”€â–¶â”‚  Fabric Eventstream     â”‚```

â”‚  Simulator      â”‚      â”‚  eh-sap-idocs    â”‚      â”‚  trd-stream-sapidocs    â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  Simulateur     â”‚â”€â”€â”€â”€â”€â–¶â”‚  Azure Event Hub â”‚â”€â”€â”€â”€â”€â–¶â”‚  Fabric Eventstream     â”‚

                                                               â”‚â”‚  Python         â”‚      â”‚  idoc-events     â”‚      â”‚  evs-sap-idoc-ingest    â”‚

                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                              â”‚                                              â”‚                                                               â”‚

                              â–¼                                              â–¼                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚                                              â”‚

                     â”‚  KQL Database       â”‚                   â”‚   Lakehouse          â”‚                              â–¼                                              â–¼

                     â”‚  kqldbsapidoc       â”‚                   â”‚   (optional)         â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

                     â”‚  - Real-time query  â”‚                   â”‚   - Long-term store  â”‚                     â”‚  KQL Database       â”‚                   â”‚   Lakehouse          â”‚

                     â”‚  - KQL analysis     â”‚                   â”‚   - Archives         â”‚                     â”‚  kqldb-sap-idoc     â”‚                   â”‚   lh-sap-idoc        â”‚

                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚  - Analyse RTI      â”‚                   â”‚   - Stockage LT      â”‚

                                â”‚                     â”‚  - RequÃªtes KQL     â”‚                   â”‚   - Archives         â”‚

                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                     â”‚                     â”‚                                â”‚

                     â–¼                     â–¼                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚                     â”‚

            â”‚  Power BI       â”‚   â”‚  Data Activator  â”‚                     â–¼                     â–¼

            â”‚  Dashboards     â”‚   â”‚  Alerts          â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  Power BI       â”‚   â”‚  Data Activator  â”‚

```            â”‚  Dashboards     â”‚   â”‚  Alertes         â”‚

            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

## ðŸ“ Folder Structure```



```## ðŸ“ Structure du dossier

fabric/

â”œâ”€â”€ README.md                      # This file```

â”œâ”€â”€ README_KQL_QUERIES.md          # KQL query collectionfabric/

â”œâ”€â”€ eventstream/â”œâ”€â”€ README.md                      # Ce fichier

â”‚   â”œâ”€â”€ EVENTSTREAM_SETUP.md       # Eventstream setup guideâ”œâ”€â”€ README_KQL_QUERIES.md          # Collection de requÃªtes KQL

â”‚   â””â”€â”€ setup-fabric-connection.ps1 # Setup scriptâ”œâ”€â”€ eventstream/

â”œâ”€â”€ data-engineering/â”‚   â”œâ”€â”€ EVENTSTREAM_SETUP.md       # Guide configuration Eventstream

â”‚   â”œâ”€â”€ notebooks/                 # Fabric notebooks (transformations)â”‚   â””â”€â”€ setup-fabric-connection.ps1 # Script de prÃ©paration

â”‚   â””â”€â”€ pipelines/                 # Data pipelinesâ”œâ”€â”€ data-engineering/

â””â”€â”€ warehouse/â”‚   â”œâ”€â”€ notebooks/                 # Notebooks Fabric (transformations)

    â””â”€â”€ schema/                    # Table schemasâ”‚   â””â”€â”€ pipelines/                 # Data pipelines

        â”œâ”€â”€ recreate-idoc-table-optimized.kqlâ””â”€â”€ warehouse/

        â”œâ”€â”€ validate-ingestion.kql    â””â”€â”€ schema/                    # SchÃ©mas de tables

        â””â”€â”€ diagnose-mapping-issue.kql```

```

## ðŸš€ DÃ©marrage rapide

## ðŸš€ Quick Start

### PrÃ©requis

### Prerequisites

âœ… Azure Event Hub dÃ©ployÃ© et opÃ©rationnel (`eh-idoc-flt8076/idoc-events`)  

âœ… Azure Event Hub deployed and operational (`ehns-fabric-sap-idocs/eh-sap-idocs`)  âœ… Messages IDoc envoyÃ©s depuis le simulateur  

âœ… IDoc messages sent from simulator  âœ… Workspace Microsoft Fabric avec capacitÃ© F64 (ou supÃ©rieure)  

âœ… Microsoft Fabric workspace with capacity (F64+)  âœ… Permissions : Contributor sur workspace + Event Hubs Data Receiver  

âœ… Permissions: Contributor on workspace + Event Hubs Data Receiver  

### Ã‰tape 1 : PrÃ©parer la connexion

### Step 1: Prepare Connection

ExÃ©cutez le script de configuration pour crÃ©er le consumer group et vÃ©rifier les permissions :

Run the configuration script to create consumer group and verify permissions:

```powershell

```powershellcd fabric\eventstream

cd fabric\eventstream.\setup-fabric-connection.ps1

.\setup-fabric-connection.ps1```

```

Ce script :

This script:- CrÃ©e le consumer group `fabric-consumer`

- Creates consumer group `fabric-consumer`- VÃ©rifie/assigne les permissions RBAC

- Verifies/assigns RBAC permissions- Affiche les informations de connexion

- Displays connection information

### Ã‰tape 2 : CrÃ©er l'Eventstream

### Step 2: Create Eventstream

1. Ouvrez votre workspace Fabric

1. Open your Fabric workspace2. CrÃ©ez un **Eventstream** : `evs-sap-idoc-ingest`

2. Create an **Eventstream**: `trd-stream-sapidocs-eventstream`3. Suivez le guide dÃ©taillÃ© : [`eventstream/EVENTSTREAM_SETUP.md`](./eventstream/EVENTSTREAM_SETUP.md)

3. Follow detailed guide: [`eventstream/EVENTSTREAM_SETUP.md`](./eventstream/EVENTSTREAM_SETUP.md)

### Ã‰tape 3 : CrÃ©er la KQL Database

### Step 3: Create KQL Database

1. Dans Fabric, crÃ©ez une **KQL Database** : `kqldb-sap-idoc`

1. In Fabric, create a **KQL Database**: `kqldbsapidoc`2. Ajoutez une destination depuis l'Eventstream vers cette database

2. Add destination from Eventstream to this database3. CrÃ©ez la table `idoc_raw` avec le schema dÃ©tectÃ©

3. Create table `idoc_raw` with auto-detected schema

### Ã‰tape 4 : Analyser les donnÃ©es

### Step 4: Analyze Data

Utilisez les requÃªtes KQL du fichier [`README_KQL_QUERIES.md`](./README_KQL_QUERIES.md) :

Use KQL queries from [`README_KQL_QUERIES.md`](./README_KQL_QUERIES.md):

```kql

```kql// AperÃ§u des derniers messages

// Latest messages overviewidoc_raw

idoc_raw| take 10

| take 10| order by timestamp desc

| order by timestamp desc| project timestamp, idoc_type, message_type, sap_system

| project timestamp, message_type, sap_system```

```

```kql

## ðŸ“Š Key Use Cases// Distribution des types d'IDoc

idoc_raw

### 1. Real-time Monitoring| summarize count() by message_type

| render piechart

**Dashboard**: Volume, latency, errors```

```kql

idoc_raw## ðŸ“Š Cas d'usage

| where timestamp > ago(1h)

| summarize count() by bin(timestamp, 5m), message_type### 1. Monitoring en temps rÃ©el

| render timechart

```**Objectif** : Surveiller le flux de messages SAP en temps rÃ©el



### 2. Business Intelligence**RequÃªte KQL** :

```kql

**Power BI**: idoc_raw

- Volume by IDoc type (pie chart)| where timestamp > ago(5m)

- Hourly trend (line chart)| summarize Messages = count() by bin(timestamp, 30s), message_type

- Top customers/products (bar chart)| render timechart

- Latency KPIs (card)```



### 3. Anomaly Detection**Dashboard** : CrÃ©ez un tile Fabric rafraÃ®chi toutes les 30 secondes



**Data Activator**:---

- Error messages (`status != "03"`)

- Abnormal volume (`> 2x average`)### 2. Analyse des performances

- High latency (`> 5 min`)

**Objectif** : Identifier les goulots d'Ã©tranglement

### 4. Long-term Archiving

**RequÃªte KQL** :

**Lakehouse**:```kql

- Partitioning: YYYY/MM/DDidoc_raw

- Format: Delta/Parquet| extend ingestion_time = ingestion_time()

- Retention: Unlimited| extend latency_seconds = datetime_diff('second', ingestion_time, todatetime(timestamp))

| summarize 

## ðŸ”§ Components    Latence_P50 = percentile(latency_seconds, 50),

    Latence_P95 = percentile(latency_seconds, 95)

### Eventstream    by bin(timestamp, 5m)

| render timechart

**Name**: `trd-stream-sapidocs-eventstream````



**Source**:---

- Azure Event Hub: `ehns-fabric-sap-idocs/eh-sap-idocs`

- Consumer group: `fabric-consumer`### 3. DÃ©tection d'anomalies

- Authentication: Entra ID

**Objectif** : Alerter sur les messages en erreur

**Destinations**:

1. **KQL Database**: `kqldbsapidoc` (real-time queries)**RequÃªte KQL** :

2. **Lakehouse**: Long-term storage (optional)```kql

3. **Reflex**: Alerts (optional)idoc_raw

| where tostring(control.status) != "03"

### KQL Database| project timestamp, message_type, docnum=control.docnum, status=control.status

| order by timestamp desc

**Name**: `kqldbsapidoc````



**Main Table**: `idoc_raw`**Alerte** : Configurez Data Activator / Reflex pour dÃ©clencher une alerte Teams/Email



**Schema**:---

```kql

.show table idoc_raw schema### 4. Analyse mÃ©tier

```

**Objectif** : Analyser les commandes par client

**Key Columns**:

- `message_type` (string): IDoc type (ORDERS, DESADV, etc.)**RequÃªte KQL** :

- `timestamp` (datetime): Message timestamp```kql

- `system_id` (string): SAP systemidoc_raw

- `document_number` (string): Document number| where message_type == "ORDERS05"

- `control` (dynamic): Control segment| extend NumCommande = control.docnum

- `data` (dynamic): IDoc data payload| extend Client = tostring(data.E1EDK01[0].BELNR)

| summarize Total_Commandes = count() by Client

### Data Engineering| top 10 by Total_Commandes desc

| render columnchart

**Notebooks**: Transformations and enrichment```

- Extract business fields

- Join with reference data---

- Create aggregated views

## ðŸ”§ Configuration avancÃ©e

**Pipelines**: Scheduled transformations

- Bronze â†’ Silver â†’ Gold### Lakehouse pour archivage

- Data quality checks

- Incremental loadsPour un stockage long terme, ajoutez une destination Lakehouse :



## ðŸ“ˆ Analytics Examples1. CrÃ©ez un **Lakehouse** : `lh-sap-idoc`

2. Depuis l'Eventstream, ajoutez destination â†’ Lakehouse

### Volume Analysis3. Table : `idoc_events` (mode Append)

4. Partitioning : Par date (annÃ©e/mois/jour)

```kql

idoc_raw### Data pipeline pour transformations

| summarize count() by message_type

| order by count_ descPour transformer les donnÃ©es avant stockage :

| render barchart

```1. CrÃ©ez un **Data Pipeline**

2. Source : KQL Database `idoc_raw`

### Latency Tracking3. Transformations :

   - Extraction des champs mÃ©tier (numÃ©ros de commande, montants)

```kql   - Enrichissement (lookup tables, conversions)

idoc_raw   - AgrÃ©gations

| where timestamp > ago(24h)4. Destination : Warehouse ou Lakehouse

| extend ingestion_lag = datetime_diff('second', ingestion_time(), timestamp)

| summarize ### Power BI Real-time

    avg_lag = avg(ingestion_lag),

    p95_lag = percentile(ingestion_lag, 95)CrÃ©ez un rapport Power BI connectÃ© au KQL Database :

    by bin(timestamp, 1h)

| render timechart1. Power BI Desktop â†’ Get Data â†’ **KQL Database**

```2. Connexion : `kqldb-sap-idoc`

3. Mode : **DirectQuery** (pour temps rÃ©el)

### Error Detection4. CrÃ©ez des visuels (cartes, graphiques, tableaux)

5. Publiez sur Fabric avec refresh automatique

```kql

idoc_raw---

| extend status_code = tostring(control.status)

| where status_code != "03"## ðŸ“– Documentation

| summarize errors = count() by message_type, status_code

| order by errors desc| Fichier | Description |

```|---------|-------------|

| [EVENTSTREAM_SETUP.md](./eventstream/EVENTSTREAM_SETUP.md) | Guide complet configuration Eventstream |

### Top Customers| [README_KQL_QUERIES.md](./README_KQL_QUERIES.md) | Collection de 50+ requÃªtes KQL |

| [setup-fabric-connection.ps1](./eventstream/setup-fabric-connection.ps1) | Script PowerShell de prÃ©paration |

```kql

idoc_raw## ðŸ” Troubleshooting

| where message_type == "ORDERS"

| extend customer = tostring(data.customer_id)### âŒ Eventstream ne reÃ§oit pas de donnÃ©es

| summarize order_count = count() by customer

| top 10 by order_count**Causes possibles** :

| render barchart- Consumer group partagÃ© avec le CLI reader

```- Permissions RBAC manquantes

- Event Hub vide

## ðŸ”” Alert Configuration

**Solution** :

### Data Activator Reflex```powershell

# CrÃ©er un consumer group dÃ©diÃ©

**Alert Name**: `IDoc Error Detection`.\eventstream\setup-fabric-connection.ps1



**Condition**:# VÃ©rifier qu'il y a des messages

```kqlcd ..\simulator

idoc_rawpython read_eventhub.py --max 5

| extend status = tostring(control.status)```

| where status != "03"

| count---

```

### âŒ Erreur de parsing JSON

**Threshold**: > 0 errors in 5 minutes

**Cause** : Format de donnÃ©es incorrect dans la source

**Action**: Send Teams notification

**Solution** :

## ðŸ“š Resources1. Dans Eventstream, Ã©ditez la source

2. Data format : **JSON**

### Documentation3. Testez avec un message : `python read_eventhub.py --max 1 --details`



| Document | Description |---

|----------|-------------|

| [EVENTSTREAM_SETUP.md](./eventstream/EVENTSTREAM_SETUP.md) | Detailed Eventstream configuration |### âŒ Latence Ã©levÃ©e

| [README_KQL_QUERIES.md](./README_KQL_QUERIES.md) | 50+ KQL query examples |

| [../SETUP_GUIDE.md](../SETUP_GUIDE.md) | Complete setup guide |**Cause** : Consumer lag, partition skew

| [../FABRIC_QUICKSTART.md](../FABRIC_QUICKSTART.md) | Quick start (5 min) |

**Solution** :

### Microsoft Learn```kql

// VÃ©rifier la latence

- [Microsoft Fabric](https://learn.microsoft.com/fabric/)idoc_raw

- [Real-Time Intelligence](https://learn.microsoft.com/fabric/real-time-intelligence/)| extend ingestion_time = ingestion_time()

- [Eventstream](https://learn.microsoft.com/fabric/real-time-intelligence/event-streams/overview)| extend latency = datetime_diff('second', ingestion_time, todatetime(timestamp))

- [KQL Database](https://learn.microsoft.com/fabric/real-time-intelligence/create-database)| summarize avg(latency), percentile(latency, 95)

- [KQL Language](https://learn.microsoft.com/azure/data-explorer/kusto/query/)```



## ðŸ§ª TestingAugmentez le nombre de partitions si nÃ©cessaire.



### Validate Ingestion---



```powershell## ðŸŽ¯ Prochaines Ã©tapes

# Send test messages

cd simulator1. âœ… **Configurer l'Eventstream** â†’ [`EVENTSTREAM_SETUP.md`](./eventstream/EVENTSTREAM_SETUP.md)

python main.py2. ðŸ“Š **Analyser avec KQL** â†’ [`README_KQL_QUERIES.md`](./README_KQL_QUERIES.md)

3. ðŸ“ˆ **CrÃ©er un dashboard Power BI** temps rÃ©el

# Verify in Fabric4. ðŸ”” **Configurer des alertes** avec Data Activator

# Open KQL Queryset and run:5. ðŸ—„ï¸ **Archiver dans Lakehouse** pour le long terme

idoc_raw | count6. ðŸ”„ **Transformer avec pipelines** pour enrichissement

// Expected: > 0 messages

```---



### Performance Test## ðŸ“š Ressources



```kql- [Microsoft Fabric - Documentation officielle](https://learn.microsoft.com/fabric/)

// Check ingestion rate- [Eventstream - Guide](https://learn.microsoft.com/fabric/real-time-intelligence/event-streams/overview)

idoc_raw- [KQL Database - Tutorial](https://learn.microsoft.com/fabric/real-time-intelligence/create-database)

| summarize count() by bin(ingestion_time(), 1m)- [KQL Query Language](https://learn.microsoft.com/azure/data-explorer/kusto/query/)

| render timechart- [Power BI Real-time](https://learn.microsoft.com/fabric/real-time-intelligence/power-bi-data-connector)

```

---

### Data Quality

## ðŸ’¡ Support

```kql

// Check for missing fieldsPour toute question ou problÃ¨me :

idoc_raw1. Consultez la section Troubleshooting ci-dessus

| summarize 2. VÃ©rifiez les logs dans Eventstream (onglet Metrics)

    total = count(),3. Testez la connexion avec le CLI : `python ..\simulator\read_eventhub.py`

    missing_type = countif(isempty(message_type)),- Eventstream configuration

    missing_timestamp = countif(isnull(timestamp))- Data Engineering pipelines (Spark)

```- SQL Warehouse schema



## ðŸŽ¯ Next Steps## Structure



1. âœ… Configure Eventstream (follow EVENTSTREAM_SETUP.md)```

2. âœ… Create KQL Databasefabric/

3. ðŸ“Š Build Power BI dashboardâ”œâ”€â”€ eventstream/

4. ðŸ”” Set up Data Activator alertsâ”‚   â””â”€â”€ eventstream-config.json       # Eventstream configuration

5. ðŸ—„ï¸ Configure Lakehouse archivingâ”œâ”€â”€ data-engineering/

6. ðŸ”„ Create data transformation pipelineâ”‚   â”œâ”€â”€ notebooks/

â”‚   â”‚   â”œâ”€â”€ bronze_to_silver.ipynb    # Bronze â†’ Silver transformation

## ðŸ’¡ Tipsâ”‚   â”‚   â”œâ”€â”€ silver_to_gold.ipynb      # Silver â†’ Gold transformation

â”‚   â”‚   â””â”€â”€ data_quality_checks.ipynb # Quality validation

### Performance Optimizationâ”‚   â””â”€â”€ pipelines/

â”‚       â”œâ”€â”€ ingestion_pipeline.json   # Main ingestion pipeline

- Use **materialized views** for frequently queried aggregationsâ”‚       â””â”€â”€ transformation_pipeline.json

- Enable **row-level security** if neededâ””â”€â”€ warehouse/

- Partition Lakehouse by date for efficient queries    â””â”€â”€ schema/

- Use **update policies** for automatic transformations        â”œâ”€â”€ bronze_tables.sql         # Bronze layer DDL

        â”œâ”€â”€ silver_tables.sql         # Silver layer DDL

### Best Practices        â”œâ”€â”€ gold_dimensions.sql       # Gold dimension tables

        â””â”€â”€ gold_facts.sql            # Gold fact tables

- Monitor consumer lag in Eventstream metrics```

- Set retention policies on KQL tables

- Use **incremental refresh** in Power BI## Prerequisites

- Implement **data quality checks** in pipelines

- Document custom KQL queries in shared querysets- Microsoft Fabric capacity (F64 or higher recommended)

- Fabric workspace created

## â“ Troubleshooting- Lakehouse created: `sap-idoc-lakehouse`

- SQL Warehouse created: `sap-3pl-warehouse`

### No Data in KQL Database- Service Principal or User account with permissions



**Check**:## Setup Instructions

1. Eventstream data preview shows messages

2. Destination is active### 1. Eventstream Setup

3. Consumer group is `fabric-consumer`

4. RBAC permissions are correct1. Navigate to your Fabric workspace

2. Create a new Eventstream: `sap-idoc-ingest`

**Solution**:3. Configure source (Event Hub)

```powershell4. Apply transformation from `eventstream/eventstream-config.json`

# Re-run setup script5. Set destination to Lakehouse

.\fabric\eventstream\setup-fabric-connection.ps1

```### 2. Lakehouse Configuration



### High Latency```bash

# Create Lakehouse using Fabric API or UI

**Check**:# Name: sap-idoc-lakehouse

1. Event Hub metrics (incoming/outgoing messages)```

2. Eventstream consumer lag

3. KQL Database ingestion queueRun DDL scripts in order:

1. `warehouse/schema/bronze_tables.sql`

**Solution**:2. `warehouse/schema/silver_tables.sql`

- Increase Event Hub throughput units3. `warehouse/schema/gold_dimensions.sql`

- Check for throttling in Azure Portal4. `warehouse/schema/gold_facts.sql`

- Verify network connectivity

### 3. Data Engineering Pipelines

### Query Performance

Import notebooks:

**Optimize**:1. Upload notebooks to Fabric workspace

```kql2. Attach to Lakehouse

// Use specific time ranges3. Configure Spark settings

idoc_raw

| where timestamp > ago(1h)  // Instead of full scanSchedule pipelines:

| where message_type == "ORDERS"  // Filter early- Bronze to Silver: Every 5 minutes

| project timestamp, document_number  // Select only needed columns- Silver to Gold: Every 15 minutes

```- Data Quality: Hourly



---## Medallion Architecture



**Ready to start?** Open [`EVENTSTREAM_SETUP.md`](./eventstream/EVENTSTREAM_SETUP.md) for step-by-step instructions!### Bronze Layer (Raw)

- Purpose: Store raw IDoc messages
- Format: Delta Lake
- Retention: 90 days
- Partitioning: By date and IDoc type

### Silver Layer (Cleansed)
- Purpose: Cleaned and normalized data
- Format: Delta Lake
- Retention: 2 years
- Features: Deduplication, validation, standardization

### Gold Layer (Analytics)
- Purpose: Business-ready dimensional model
- Format: SQL Warehouse tables
- Retention: 7 years
- Design: Star schema with dimensions and facts

## Key Tables

| Table | Layer | Description |
|-------|-------|-------------|
| `bronze_idocs` | Bronze | Raw IDoc messages |
| `silver_shipments` | Silver | Cleansed shipment data |
| `silver_deliveries` | Silver | Cleansed delivery data |
| `dim_customer` | Gold | Customer dimension |
| `dim_location` | Gold | Location dimension |
| `fact_shipment` | Gold | Shipment fact table |

## Data Transformation Flow

```
Event Hub
    â†“
Eventstream (validation, enrichment)
    â†“
bronze_idocs (raw storage)
    â†“
Spark Notebook: bronze_to_silver
    â†“
silver_* tables (cleansed data)
    â†“
Spark Notebook: silver_to_gold
    â†“
dim_* & fact_* tables (analytics)
```

## Monitoring

- Use Fabric Monitoring for Eventstream metrics
- Check pipeline execution history
- Monitor Delta Lake table metrics
- Set up alerts for pipeline failures

## Performance Optimization

### Delta Lake Optimization

```sql
-- Optimize tables regularly
OPTIMIZE bronze_idocs ZORDER BY (idoc_type, processing_date);
OPTIMIZE silver_shipments ZORDER BY (customer_id, ship_date);

-- Vacuum old versions
VACUUM bronze_idocs RETAIN 168 HOURS;
VACUUM silver_shipments RETAIN 168 HOURS;
```

### Spark Configuration

```python
# Recommended Spark settings for notebooks
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")
spark.conf.set("spark.databricks.delta.autoCompact.enabled", "true")
```

## Troubleshooting

### Eventstream Issues
- Check Event Hub connectivity
- Verify schema validation rules
- Review error logs in monitoring

### Pipeline Failures
- Check Spark logs
- Verify table permissions
- Ensure sufficient capacity

### Performance Issues
- Review partition strategy
- Optimize Delta tables
- Increase Spark compute resources

## Development Workflow

1. Develop transformations in notebooks
2. Test with sample data
3. Validate data quality
4. Deploy to production workspace
5. Schedule pipelines

## Contributing

When modifying Fabric components:
1. Test in development workspace
2. Document changes in notebook markdown
3. Update this README
4. Create backup of current configuration
5. Deploy changes incrementally

## Support

- Fabric Documentation: https://learn.microsoft.com/fabric/
- Internal Support: data-engineering@company.com
